{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Processing: (7134, 89)\n",
      "After Processing: (7134, 61)\n",
      "\n",
      "Final Report:\n",
      "Confusion_matrix: \n",
      "[[937  41]\n",
      " [ 35 996]]\n",
      "\n",
      "Classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       978\n",
      "           1       0.96      0.97      0.96      1031\n",
      "\n",
      "    accuracy                           0.96      2009\n",
      "   macro avg       0.96      0.96      0.96      2009\n",
      "weighted avg       0.96      0.96      0.96      2009\n",
      "\n",
      "Accuracy:  96.21702339 %\n",
      "Precision:  96.04628737 %\n",
      "Recall:  96.60523763 %\n",
      "F1 Score:  96.32495164 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import  confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Training data\n",
    "training_data = pd.read_csv(\"training.csv\")\n",
    "\n",
    "# Converting phishing to 1 and legitimate to 0 for training data\n",
    "training_data['status'] = training_data['status'].map({'phishing': 1, 'legitimate': 0})\n",
    "print(\"Before Processing:\",training_data.shape)\n",
    "\n",
    "# Preprocessing\n",
    "training_data = training_data.drop('url', axis=1)\n",
    "training_data = training_data.fillna(0)\n",
    "\n",
    "# So the Top features contributing to the model are taken into consideration and the rest are ignored\n",
    "# Top 61 features contributing to the model are considered\n",
    "training_data = training_data[['google_index', 'page_rank', 'nb_hyperlinks', 'web_traffic', 'domain_age', 'nb_www', 'safe_anchor', 'length_url', 'ratio_digits_url', 'shortest_word_host', 'domain_registration_length', 'longest_words_raw', 'phish_hints', 'length_hostname', 'char_repeat', 'shortest_word_path', 'nb_slash', 'domain_in_title', 'shortest_words_raw', 'nb_dots', 'ratio_digits_host', 'longest_word_host', 'nb_hyphens', 'ip', 'nb_qm', 'nb_subdomains', 'domain_with_copyright', 'nb_redirection', 'domain_in_brand', 'https_token', 'nb_underscore', 'prefix_suffix', 'shortening_service', 'nb_and', 'nb_com', 'nb_percent', 'suspecious_tld', 'whois_registered_domain', 'random_domain', 'statistical_report', 'tld_in_subdomain', 'dns_record', 'nb_space', 'tld_in_path', 'nb_at', 'nb_colon', 'http_in_path', 'nb_semicolumn', 'nb_dslash', 'nb_tilde', 'brand_in_path', 'port', 'brand_in_subdomain', 'nb_comma', 'nb_external_redirection', 'path_extension', 'punycode', 'nb_dollar', 'nb_star', 'nb_or', 'status']]\n",
    "\n",
    "print(\"After Processing:\", training_data.shape)\n",
    "\n",
    "# Split the data into x = features and y = label\n",
    "x_train = training_data.iloc[:, 0:-1]\n",
    "y_train = training_data.iloc[:, -1]\n",
    "\n",
    "# Model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# Model training\n",
    "clf.fit(x_train, y_train)\n",
    "# Model is trained then test the model\n",
    "\n",
    "# Testing data same process as training data\n",
    "testing_data = pd.read_csv(\"testing.csv\")\n",
    "testing_data['status'] = testing_data['status'].map({'phishing': 1, 'legitimate': 0})\n",
    "testing_data = testing_data.drop('url', axis=1)\n",
    "testing_data = testing_data.fillna(0)\n",
    "testing_data = testing_data[['google_index', 'page_rank', 'nb_hyperlinks', 'web_traffic', 'domain_age', 'nb_www', 'safe_anchor', 'length_url', 'ratio_digits_url', 'shortest_word_host', 'domain_registration_length', 'longest_words_raw', 'phish_hints', 'length_hostname', 'char_repeat', 'shortest_word_path', 'nb_slash', 'domain_in_title', 'shortest_words_raw', 'nb_dots', 'ratio_digits_host', 'longest_word_host', 'nb_hyphens', 'ip', 'nb_qm', 'nb_subdomains', 'domain_with_copyright', 'nb_redirection', 'domain_in_brand', 'https_token', 'nb_underscore', 'prefix_suffix', 'shortening_service', 'nb_and', 'nb_com', 'nb_percent', 'suspecious_tld', 'whois_registered_domain', 'random_domain', 'statistical_report', 'tld_in_subdomain', 'dns_record', 'nb_space', 'tld_in_path', 'nb_at', 'nb_colon', 'http_in_path', 'nb_semicolumn', 'nb_dslash', 'nb_tilde', 'brand_in_path', 'port', 'brand_in_subdomain', 'nb_comma', 'nb_external_redirection', 'path_extension', 'punycode', 'nb_dollar', 'nb_star', 'nb_or', 'status']]\n",
    "\n",
    "# Testing data\n",
    "x_test = testing_data.iloc[:, 0:-1]\n",
    "y_test = testing_data.iloc[:, -1]\n",
    "\n",
    "# Predicting the model and checking with the actual values\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "# Final Report\n",
    "print()\n",
    "print(\"Final Report:\")\n",
    "print(\"Confusion_matrix: \")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print()\n",
    "print(\"Classification_report: \")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy = 100 * accuracy\n",
    "print(\"Accuracy: \", round(accuracy, 8), \"%\")\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "precision = 100 * precision\n",
    "print(\"Precision: \", round(precision, 8), \"%\")\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "recall = 100 * recall\n",
    "print(\"Recall: \", round(recall, 8), \"%\")\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "f1 = 100 * f1\n",
    "print(\"F1 Score: \", round(f1, 8), \"%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# My Observations on the given dataset\n",
    "# Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features\n",
    "# Features with low correlation are less linearly dependent and hence have almost no effect on the dependent variable. So, when two features have low correlation, we can drop both of the two features\n",
    "\n",
    "# if length of the hostname is more than 21 then there is a more possible that it is in the phising class\n",
    "# nb_semicolon, nb_eq_bins, nb_underscore, nb_percent, nb_ratio_digits_hosts do not contribute to the classification much as the phising and legitimate contribution are almost equal\n",
    "# if nb_dots > 3 , nb_slash > 4, rtio_digits_url > 0.0221, longest_words_raw > 12, longest_words_path > 14, avg_word_host > 9, avg_word_path > 6 then there is large possibility that it is in the phishing class.\n",
    "# nb_semicolon, nb_eq_bins, nb_underscore, nb_percent, nb_ratio_digits_hosts do not contribute as much as the phising and legitimate contribution are almost equal.\n",
    "\n",
    "# By Taking the coorelation between the features \n",
    "# avg_words_raw and longest_words_raw are highly coorelated with each other.. so we can drop one of them\n",
    "# abnormal_subdomain and ratio_digits_host are positively coorelated with each other.. so we can drop one of them\n",
    "# avg_words_path correlated with longest_words_host, longest_word_path, avg_words_raw.. so we can drop one of them\n",
    "# nb_eq is highly coorelated with nb_qm , nb_and and length_words_raw.. so we can drop nb_eq\n",
    "# avg_words_host, shortest_word_host, longest_word_host are highly coorelated.. so we can drop avg_words_host\n",
    "\n",
    "# When the google index is 0, Legitimate classs are more but when the Phising class holds true when google index is 1. \n",
    "# When we have dns_record value as 1, mostly it corresponds to phishing class.\n",
    "# When domain_age is high then it is more of a legitimate class as phishing class.\n",
    "# if nb_hyperlinks is high, then it is probably a factor for suspecting as phishing class\n",
    "# ratio_extredirection and ext_error ratio are positively coorelated with phishing\n",
    "\n",
    "# After some trial's , I found the features domain_with_copyright, nb_hyperlinks, safe_anchor is useful for getting high accuracy.\n",
    "# By taking the coorelation, I have removed the features which are not contributing much to the classification\n",
    "# By all the above methods, I have reduced the number of features from 89 to 61\n",
    "# So only 61 features are contributing to the classification of the dataset for high accuracy\n",
    "\n",
    "# Finally only 61 features are used for training the model and the rest of the features are ignored"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
