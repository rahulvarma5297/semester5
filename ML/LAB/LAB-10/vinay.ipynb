{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 145\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mfor\u001b[39;00m degree \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m6\u001b[39m, \u001b[39m9\u001b[39m, \u001b[39m12\u001b[39m, \u001b[39m15\u001b[39m]:\n\u001b[0;32m    144\u001b[0m       m, c \u001b[39m=\u001b[39m non_linear_regression(X, Y, learning_rate, iterations, degree)\n\u001b[1;32m--> 145\u001b[0m       plot_model(X, Y, m, c, degree)\n\u001b[0;32m    147\u001b[0m \u001b[39m# Plot the Lasso (L1) and Ridge (L2) regression models for lambda values [1e-10,1e-8,1e-4,1e-2,1,10,20],\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[39m# and print the SSE, Coefficients for the plotted models.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m\n",
      "Cell \u001b[1;32mIn [3], line 101\u001b[0m, in \u001b[0;36mplot_model\u001b[1;34m(X, Y, learning_rate, iterations, degree)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_model\u001b[39m(X, Y, learning_rate, iterations, degree):\n\u001b[0;32m     99\u001b[0m     \u001b[39m# Linear Regression\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[39mif\u001b[39;00m degree \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 101\u001b[0m         m, c \u001b[39m=\u001b[39m linear_regression(X, Y, learning_rate, iterations)\n\u001b[0;32m    102\u001b[0m         Y_pred \u001b[39m=\u001b[39m m \u001b[39m*\u001b[39m X \u001b[39m+\u001b[39m c\n\u001b[0;32m    103\u001b[0m         plt\u001b[39m.\u001b[39mplot(X, Y_pred, color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mred\u001b[39m\u001b[39m'\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLinear Regression\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [3], line 22\u001b[0m, in \u001b[0;36mlinear_regression\u001b[1;34m(X, Y, learning_rate, iterations)\u001b[0m\n\u001b[0;32m     20\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(X)\n\u001b[0;32m     21\u001b[0m \u001b[39m# Performing Gradient Descent\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39;49m(iterations):\n\u001b[0;32m     23\u001b[0m       Y_pred \u001b[39m=\u001b[39m m \u001b[39m*\u001b[39m X \u001b[39m+\u001b[39m c\n\u001b[0;32m     24\u001b[0m       D_m \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m \u001b[39m/\u001b[39m n) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m(X \u001b[39m*\u001b[39m (Y \u001b[39m-\u001b[39m Y_pred))\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.arange(60, 300, 1)\n",
    "X = np.radians(X)\n",
    "\n",
    "K = np.random.normal(0, 0.15, X.shape)\n",
    "Y = np.sin(X) + K\n",
    "\n",
    "X = X.reshape(-1, 1)\n",
    "Y = Y.reshape(-1, 1)\n",
    "\n",
    "# Normalizing the X values using z-score\n",
    "X = (X - np.mean(X)) / np.std(X)\n",
    "\n",
    "# Implementing Linear regression from scratch\n",
    "def linear_regression(X, Y, learning_rate, iterations):\n",
    "      # Initialize the parameters\n",
    "      m = 0\n",
    "      c = 0\n",
    "      n = len(X)\n",
    "      # Performing Gradient Descent\n",
    "      for i in range(iterations):\n",
    "            Y_pred = m * X + c\n",
    "            D_m = (-2 / n) * sum(X * (Y - Y_pred))\n",
    "            D_c = (-2 / n) * sum(Y - Y_pred)\n",
    "            m = m - learning_rate * D_m\n",
    "            c = c - learning_rate * D_c\n",
    "      return m, c\n",
    "\n",
    "# Implementing non-linear regression from scratch\n",
    "# Using Gradient Descent\n",
    "\n",
    "def non_linear_regression(X, Y, learning_rate, iterations, degree):\n",
    "      # Initialize the parameters\n",
    "      m = np.zeros((degree, 1))\n",
    "      c = 0\n",
    "      n = len(X)\n",
    "      # Performing Gradient Descent\n",
    "      for i in range(iterations):\n",
    "            Y_pred = np.zeros((n, 1))\n",
    "            for j in range(degree):\n",
    "                  Y_pred += m[j] * (X ** (j + 1))\n",
    "            Y_pred += c\n",
    "            D_m = np.zeros((degree, 1))\n",
    "            for j in range(degree):\n",
    "                  D_m[j] = (-2 / n) * sum((X ** (j + 1)) * (Y - Y_pred))\n",
    "            D_c = (-2 / n) * sum(Y - Y_pred)\n",
    "            m = m - learning_rate * D_m\n",
    "            c = c - learning_rate * D_c\n",
    "      return m, c\n",
    "\n",
    "# Implementing Lasso regression from scratch\n",
    "# Using Gradient Descent\n",
    "\n",
    "def lasso_regression(X, Y, learning_rate, iterations, degree, l):\n",
    "      # Initialize the parameters\n",
    "      m = np.zeros((degree, 1))\n",
    "      c = 0\n",
    "      n = len(X)\n",
    "      # Performing Gradient Descent\n",
    "      for i in range(iterations):\n",
    "            Y_pred = np.zeros((n, 1))\n",
    "            for j in range(degree):\n",
    "                  Y_pred += m[j] * (X ** (j + 1))\n",
    "            Y_pred += c\n",
    "            D_m = np.zeros((degree, 1))\n",
    "            for j in range(degree):\n",
    "                  D_m[j] = (-2 / n) * sum((X ** (j + 1)) * (Y - Y_pred)) + l * np.sign(m[j])\n",
    "            D_c = (-2 / n) * sum(Y - Y_pred)\n",
    "            m = m - learning_rate * D_m\n",
    "            c = c - learning_rate * D_c\n",
    "      return m, c\n",
    "\n",
    "# Implementing Ridge regression from scratch\n",
    "# Using Gradient Descent\n",
    "\n",
    "def ridge_regression(X, Y, learning_rate, iterations, degree, l):\n",
    "      # Initialize the parameters\n",
    "      m = np.zeros((degree, 1))\n",
    "      c = 0\n",
    "      n = len(X)\n",
    "      # Performing Gradient Descent\n",
    "      for i in range(iterations):\n",
    "            Y_pred = np.zeros((n, 1))\n",
    "            for j in range(degree):\n",
    "                  Y_pred += m[j] * (X ** (j + 1))\n",
    "            Y_pred += c\n",
    "            D_m = np.zeros((degree, 1))\n",
    "            for j in range(degree):\n",
    "                  D_m[j] = (-2 / n) * sum((X ** (j + 1)) * (Y - Y_pred)) + l * m[j]\n",
    "            D_c = (-2 / n) * sum(Y - Y_pred)\n",
    "            m = m - learning_rate * D_m\n",
    "            c = c - learning_rate * D_c\n",
    "      return m, c\n",
    "\n",
    "# Plot the created models for the power of 1, 3,6,9,12 and 15, and print the SSE, Coefficients for the plotted models.\n",
    "\n",
    "def plot_model(X, Y, learning_rate, iterations, degree):\n",
    "    # Linear Regression\n",
    "    if degree == 1:\n",
    "        m, c = linear_regression(X, Y, learning_rate, iterations)\n",
    "        Y_pred = m * X + c\n",
    "        plt.plot(X, Y_pred, color='red', label='Linear Regression')\n",
    "        print('SSE for Linear Regression: ', sum((Y - Y_pred) ** 2))\n",
    "        print('Coefficients for Linear Regression: ', m, c)\n",
    "    # Non-Linear Regression\n",
    "    else:\n",
    "        theta = non_linear_regression(X, Y, learning_rate, iterations, degree)\n",
    "        Y_pred = np.zeros((len(X), 1))\n",
    "        for i in range(degree + 1):\n",
    "            Y_pred += theta[i] * (X ** i)\n",
    "        plt.plot(X, Y_pred, label='Non-Linear Regression of degree ' + str(degree))\n",
    "        print('SSE for Non-Linear Regression of degree ' + str(degree) + ': ', sum((Y - Y_pred) ** 2))\n",
    "        print('Coefficients for Non-Linear Regression of degree ' + str(degree) + ': ', theta)\n",
    "\n",
    "# Plot the Lasso (L1) and Ridge (L2) regression models for lambda values [1e-10,1e-8,1e-4,1e-2,1,10,20],\n",
    "# and print the SSE, Coefficients for the plotted models.\n",
    "\n",
    "def plot_regularized_model(X, Y, learning_rate, iterations, degree, l):\n",
    "    # Lasso Regression\n",
    "    if l == 1:\n",
    "        theta = lasso_regression(X, Y, learning_rate, iterations, degree, l)\n",
    "        Y_pred = np.zeros((len(X), 1))\n",
    "        for i in range(degree + 1):\n",
    "            Y_pred += theta[i] * (X ** i)\n",
    "        plt.plot(X, Y_pred, label='Lasso Regression with lambda ' + str(l))\n",
    "        print('SSE for Lasso Regression with lambda ' + str(l) + ': ', sum((Y - Y_pred) ** 2))\n",
    "        print('Coefficients for Lasso Regression with lambda ' + str(l) + ': ', theta)\n",
    "    # Ridge Regression\n",
    "    else:\n",
    "        theta = ridge_regression(X, Y, learning_rate, iterations, degree, l)\n",
    "        Y_pred = np.zeros((len(X), 1))\n",
    "        for i in range(degree + 1):\n",
    "            Y_pred += theta[i] * (X ** i)\n",
    "        plt.plot(X, Y_pred, label='Ridge Regression with lambda ' + str(l))\n",
    "        print('SSE for Ridge Regression with lambda ' + str(l) + ': ', sum((Y - Y_pred) ** 2))\n",
    "        print('Coefficients for Ridge Regression with lambda ' + str(l) + ': ', theta)\n",
    "\n",
    "# Plot the created models for the power of 1, 3,6,9,12 and 15, and print the SSE, Coefficients for the plotted models.\n",
    "\n",
    "learning_rate = 0.001\n",
    "iterations = 1000\n",
    "for degree in [1, 3, 6, 9, 12, 15]:\n",
    "      m, c = non_linear_regression(X, Y, learning_rate, iterations, degree)\n",
    "      plot_model(X, Y, m, c, degree)\n",
    "\n",
    "# Plot the Lasso (L1) and Ridge (L2) regression models for lambda values [1e-10,1e-8,1e-4,1e-2,1,10,20],\n",
    "# and print the SSE, Coefficients for the plotted models.\n",
    "\n",
    "learning_rate = 0.001\n",
    "iterations = 1000\n",
    "for degree in [1, 3, 6, 9, 12, 15]:\n",
    "      for l in [1e-10, 1e-8, 1e-4, 1e-2, 1, 10, 20]:\n",
    "            m, c = lasso_regression(X, Y, learning_rate, iterations, degree, l)\n",
    "            plot_regularized_model(X, Y, m, c, degree, l)\n",
    "            m, c = ridge_regression(X, Y, learning_rate, iterations, degree, l)\n",
    "            plot_regularized_model(X, Y, m, c, degree, l)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
